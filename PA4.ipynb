{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H88UQQRR62MO"
   },
   "source": [
    "# Part 0: Introduction and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VU3U0GzS62MT"
   },
   "source": [
    "This project guides you through the general pipeline used to build and train a convolutional neural network (CNN) for an image classification task. You'll be implementing a baseline model, improving upon your baseline model, and trying to fool your CNN with adversarial images.\n",
    "\n",
    "\n",
    "We will be using a library called PyTorch which simplifies many of the low-level implementation details of neural networks for us, so that we can focus on the high-level deep learning concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EGpbjcSS62MU"
   },
   "source": [
    "\n",
    "### Using this notebook\n",
    "This notebook will walk you through the different parts of the assignment, with detailed instructions and explanations at every step. You'll see red <font color=\"red\">TODO</font> for things you need to write in `student.py`.\n",
    "\n",
    "\n",
    "### Your code\n",
    "Once again, **all of your code needs to be written in `student.py`, and not in this notebook!** We will not grade any code written in this notebook (since you will not be submitting it); this notebook is meant to serve as a central tool that you can use to run your code and visualize your outputs. It also contains detailed explanations at every step to guide you. If anything is unclear, please post on Edstem!\n",
    "\n",
    "\n",
    "### Python version\n",
    "The top-right of this notebook should display a Python version; please make sure that it says Python 3 before proceeding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4SNfksUH62MU"
   },
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "\n",
    "# Constants\n",
    "classes = [\n",
    "     'bighorn-sheep',\n",
    "     'bison',\n",
    "     'black-stork',\n",
    "     'brown-bear',\n",
    "     'bullfrog',\n",
    "     'camel',\n",
    "     'gazelle',\n",
    "     'golden-retriever',\n",
    "     'goldfish',\n",
    "     'ladybug',\n",
    "     'lion',\n",
    "     'orangutan',\n",
    "     'penguin',\n",
    "     'persian-cat',\n",
    "     'pig',\n",
    "     'puma'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8qKKabIc62MV"
   },
   "source": [
    "# Part 1: Reading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VchksGNt62MV"
   },
   "source": [
    "### Step 1: Unpacking the dataset\n",
    "\n",
    "Unzip the `data.zip` file. You should now have a folder called `data` with this structure:\n",
    "\n",
    "```\n",
    "data/\n",
    "    train/\n",
    "        bighorn-sheep/\n",
    "            XXXX.JPEG\n",
    "        goldfish/\n",
    "            XXXX.JPEG\n",
    "        ...\n",
    "    val/\n",
    "        bighorn-sheep/\n",
    "            XXXX.JPEG\n",
    "        goldfish/\n",
    "            XXXX.JPEG\n",
    "        ...\n",
    "    test/\n",
    "        bighorn-sheep/\n",
    "            XXXX.JPEG\n",
    "        goldfish/\n",
    "            XXXX.JPEG\n",
    "        ...\n",
    "```\n",
    "\n",
    "The names of the actual images (`XXXX.JPEG` in the diagram above) don't matter - only the folder structure matters, where the names of the folders under `train/`, `val/` and `test/` correspond to the class names.\n",
    "\n",
    "**Data summary:**\n",
    "There are 16 classes, each with 500 training images and 50 validation images. Each image is 64x64 with 3 channels.\n",
    "\n",
    "The classes are the following:\n",
    "```\n",
    "bighorn-sheep\n",
    "bison\n",
    "black-stork\n",
    "brown-bear\n",
    "bullfrog\n",
    "camel\n",
    "gazelle\n",
    "golden-retriever\n",
    "goldfish\n",
    "ladybug\n",
    "lion\n",
    "orangutan\n",
    "penguin\n",
    "persian-cat\n",
    "pig\n",
    "puma\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "SaB8tGUs62MW",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### Step 2: A quick sanity check...\n",
    "Before training any machine learning model, it's important to fully understand the data that is being dealt with. What does it look like, and does it match our expectations? In this part, we'll run code that looks into the dataset and shows us what it contains.\n",
    "\n",
    "To help with this, we have first defined a `visualize` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S1aNzLSK62MW"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize(N, class_name, root_folder):\n",
    "    files = os.listdir(os.path.join(root_folder, class_name))\n",
    "    files = [os.path.join(root_folder, class_name, x) for x in files]\n",
    "    files = [x for x in files if os.path.isfile(x)]\n",
    "    \n",
    "    for i in range(N):\n",
    "        img = np.array(Image.open(files[i]).convert('RGB'))\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JsEgnmv_62MX"
   },
   "outputs": [],
   "source": [
    "N = 5\n",
    "class_name = 'pig'\n",
    "folder     = os.path.join('.', 'data', 'train')\n",
    "\n",
    "print(\"Visualizing class: {}\".format(class_name))\n",
    "visualize(N, class_name, folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bchh_bxf62MX"
   },
   "source": [
    "### Step 3: Normalize data\n",
    "\n",
    "Normalizing the data means getting the features in a similar range of values. This is an important step in your CNN pipeline. Making the features (here, pixels) in a similar data distribution helps the model converge.\n",
    "\n",
    "We've already looped through the training dataset and found the channel-wise means and standard deviations for you. We've divided them by 255, since the PyTorch Tensors have values between [0,1]. The images are normalized by subtracting the means and dividing by the standard deviations.  \n",
    "\n",
    "PyTorch has an easy method for integrating normalization into your machine learning pipeline - you create \n",
    "[transforms](https://pytorch.org/docs/master/torchvision/transforms.html?highlight=transform), which are just different data manipulations you can chain together. Before you pass an image to your model, it's fed through the transform first.\n",
    "\n",
    "The transform below converts the images into the correct format and then applies the normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qq9E3_TC62MX"
   },
   "outputs": [],
   "source": [
    "dataset_means = [123./255., 116./255.,  97./255.]\n",
    "dataset_stds  = [ 54./255.,  53./255.,  52./255.]\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(dataset_means, dataset_stds)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQh5cTzY62MY"
   },
   "source": [
    "###  Create Dataset and DataLoader for PyTorch model\n",
    "The model we will soon create in PyTorch needs a way of understanding our dataset folder structure.\n",
    "\n",
    "This has 2 steps:\n",
    "\n",
    "1. Define a **[Dataset](https://pytorch.org/docs/master/data.html#torch.utils.data.Dataset)**\n",
    "    - Tells the model where your data is and how to access it\n",
    "    - Requires `__getitem__` function, which tells your model how to grab an image and label when needed\n",
    "    - Requires `__len__` function, which returns the size of the dataset\n",
    "    \n",
    "   &nbsp; \n",
    "2. Define a **[DataLoader](https://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader)**\n",
    "    - Tells the model how to sample from the dataset\n",
    "    - Defines the _batch size_, which is the number of images propagated through the network during one forward pass before a gradient update\n",
    "\n",
    "Run the cell below to create datasets and dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0mfHlwnn62MY"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "train_dataset = datasets.ImageFolder(os.path.join('.', 'data', 'train'),transform=transform)\n",
    "val_dataset   = datasets.ImageFolder(os.path.join('.', 'data', 'val'),  transform=transform)\n",
    "test_dataset   = datasets.ImageFolder(os.path.join('.', 'data', 'test'),  transform=transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader   = DataLoader(test_dataset,   batch_size=BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B82BbOBA62MZ"
   },
   "source": [
    "# Part 1: Fitting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JYIRwX6s62MZ"
   },
   "source": [
    "### High-Level Overview\n",
    "Now, we will attempt to fit the data using a convolutional neural network. This network will learn to classify input images into one of the 16 animal categories, based on the training data that we provide to it. We will use our validation dataset to get a sense of how the network performs on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6eAiJuI62MZ"
   },
   "source": [
    "### Create a baseline model\n",
    "Below we have defined a baseline model, which you will improve on in a later part of this project. There are two parts to defining a model:\n",
    "\n",
    "1. `__init__`: Define the **layers** of your network. Check out torch.nn documentation for pre-implemented layers.\n",
    "\n",
    "    &nbsp;\n",
    "2. `forward`: Chains together the layers you defined in `__init__`, creating the **pipeline** for a forward pass (i.e. where the image goes when you feed it to the network). In our baseline, the image passes through a series of **convolution** layers followed by **ReLU** (Rectified Linear Unit) nonlinearities, followed by a couple **fully connected** layers.\n",
    "\n",
    "### **<font color='red'>TODO : </font>** \n",
    "\n",
    "Implement the baseline model architecture in `student.py`. The architecture is the following:\n",
    "- **conv1**: convolution layer with 6 output channels, kernel size of 3, stride of 2, padding of 1\n",
    "- **ReLU** nonlinearity\n",
    "- **conv2**: convolution layer with 12 output channels, kernel size of 3, stride of 2, padding of 1\n",
    "- **ReLU** nonlinearity\n",
    "- **conv3**: convolution layer with 24 output channels, kernel size of 3, stride of 2, padding of 1\n",
    "- **ReLU** nonlinearity\n",
    "- **fc**:    fully connected layer with 128 output features\n",
    "- **ReLU** nonlinearity\n",
    "- **cls**:   fully connected layer with 16 output features (the number of classes)\n",
    "\n",
    "\n",
    "\n",
    "The feature map sizes in this case are calculated by dividing the input size by the _stride_ (how many pixels you slide the kernel over each time you do a convolution). For example, we started with a 64 x 64 image, passed it through the `conv1` layer with a stride of 2, giving an output size of 32 x 32. When you create your own model, it's important to pay attention to these sizes. You'll need to figure out the dimensions to the first Linear layer, which needs an input size equal to the number of pixels in your Tensor by that point in the network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6U8auMUy62MZ"
   },
   "source": [
    "To verify that we added the correct layers above, we can **load the pretrained weights** into the architecture. The weights file essentially holds a dictionary, where the keys are the layer names and the values are the parameter weights. If your architecture is correct, you should be able to print the network and see the layers.\n",
    "\n",
    "**Please define the forward pass manually rather than using nn.Sequential. Otherwise, the pretrained model weights won't load correctly.**\n",
    "\n",
    "You'll have to create your own model, so make sure you understand the baseline architecture before proceeding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qGh_Akbo62Ma"
   },
   "outputs": [],
   "source": [
    "import student\n",
    "\n",
    "net_pretrained = student.AnimalBaselineNet()\n",
    "\n",
    "# Load pretrained weights into network to check if architecture is correct\n",
    "weights_path = os.path.join('.', 'models', 'baseline.pth')\n",
    "net_pretrained.load_state_dict(torch.load(weights_path, map_location=\"cpu\"))\n",
    "\n",
    "for layer in net_pretrained.children():\n",
    "    print(layer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XDhwHE6662Ma"
   },
   "source": [
    "###  Train the model\n",
    "\n",
    "Great, we have our model and our data! There's two more steps before we can train:\n",
    "- Decide what **loss function** you need. Here, we use [_cross entropy loss_](https://pytorch.org/docs/master/nn.html#crossentropyloss), typical for image classification.\n",
    "- Decide how many **epochs** you will train your model for. One epoch means one pass of the full training dataset through your model. Here, we set it to 80. When you train your own model, you'll have to keep track of the losses to decide when to stop.\n",
    "- Decide what **optimizer** you will use. An optimizer tells your model how to take steps along the gradient to try and reach a minimum. Here, we use the popular _Adam optimizer_ (if you're curious, here's the [paper](https://arxiv.org/pdf/1412.6980.pdf)). It adapts the learning rates for each parameter based on how quickly each parameter's gradient is changing. It's known as being more forgiving for less-than-optimal hyperparameter choices than other optimizers are.\n",
    "\n",
    "We also redefine the network to start from scratch, rather than loading in pretrained weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R5yBDoCa62Ma"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "net = student.AnimalBaselineNet()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.01)\n",
    "epochs = 80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0LcmoJA62Ma"
   },
   "source": [
    "Time to train! For each epoch, the model does a forward and backward pass for training, and then just a forward pass for validation. But what are forward and backward passes?\n",
    "\n",
    "- **Forward pass**: sends a batch of images through the network. Returns the output of the last linear layer, which has 16 values: each value is the likelihood that the image belongs to that particular class.\n",
    "- **Backward pass**: calculates the loss and the gradient of the loss with respect to the model parameters. Optimizer updates the weights based on this gradient.\n",
    "\n",
    "During training, we keep track of the loss and accuracy for both training & validation phases so we can visualize our model performance after it's done.\n",
    "\n",
    "### **<font color='red'>TODO: </font>** \n",
    "Implement the `model_train` function in `student.py`. We will be running your `model_train` with your `AnimalBaselineNet` to check accuracy.\n",
    "\n",
    " \n",
    "Something to think about: What do you notice about the train vs. validation performance? What might this mean about the model?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xqU1qM1N62Ma",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Keep track of average losses, training accuracy and validation accuracy for each epoch\n",
    "train_loss_history = np.zeros(epochs)\n",
    "train_acc_history  = np.zeros(epochs)\n",
    "val_loss_history   = np.zeros(epochs)\n",
    "val_acc_history    = np.zeros(epochs)\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "     # ============================ Training ==============================\n",
    "    print(\"Training...\")\n",
    "    \n",
    "    # Place network in training mode\n",
    "    net.train()\n",
    "    \n",
    "    # Initialize running epoch loss and number correctly classified\n",
    "    running_loss   = 0.0\n",
    "    num_correct    = 0.0\n",
    "    total_images   = 0.0\n",
    "    \n",
    "    \n",
    "    for batch_num, (inputs, labels) in enumerate(train_dataloader):\n",
    "        # [inputs] and [labels] is one batch of images and their classes\n",
    "        \n",
    "        # Function call to student\n",
    "        curr_loss, curr_correct, curr_images = \\\n",
    "            student.model_train(net, inputs, labels, criterion, optimizer)\n",
    "        \n",
    "        running_loss += curr_loss\n",
    "        num_correct += curr_correct\n",
    "        total_images += curr_images\n",
    "\n",
    "    # Update statistics for epoch\n",
    "    train_loss_history[epoch] = running_loss / total_images\n",
    "    train_acc_history[epoch]  = float(num_correct)  / float(total_images)\n",
    "    print(\"Epoch: {:d} | Train Avg. Loss: [{}] | Acc: {} on {} images\\n\".format(epoch,\n",
    "          round(train_loss_history[epoch],4), train_acc_history[epoch], total_images) )\n",
    "    \n",
    "    # ============================ Validation ==============================\n",
    "    print(\"Validating...\")\n",
    "    # Place network in testing mode (won't need to keep track of gradients)\n",
    "    net.eval()\n",
    "    \n",
    "    running_loss   = 0.0\n",
    "    num_correct    = 0.0\n",
    "    total_images   = 0.0\n",
    "    \n",
    "    for batch_num, (inputs, labels) in enumerate(val_dataloader):\n",
    "        \n",
    "        # Propagate batch through network\n",
    "        outputs  = net(inputs)\n",
    "                                                 \n",
    "        # Calculate loss\n",
    "        loss     = criterion(outputs, labels.squeeze())\n",
    "                                                 \n",
    "        # Prediction is class with highest class score\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        running_loss  += loss.item()\n",
    "        num_correct   += torch.sum(preds == labels.data.reshape(-1))\n",
    "        total_images  += labels.data.numpy().size\n",
    "        \n",
    "    # Update statistics for validation data\n",
    "    val_loss_history[epoch] = running_loss / total_images\n",
    "    val_acc_history[epoch]  = float(num_correct)  / float(total_images) \n",
    "    print(\"Val Avg. Loss: [{}] Acc: {} on {} images\\n\".format(\n",
    "        round(val_loss_history[epoch],4), val_acc_history[epoch], total_images))\n",
    "    \n",
    "print(\"Time Elapsed: {} seconds\".format(\n",
    "    (datetime.now() - start_time).total_seconds()))\n",
    "\n",
    "\n",
    "\n",
    "torch.save(net.state_dict(), os.path.join('.','models','my_baseline.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RaR9Wwqj62Mb"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(11,5))\n",
    "\n",
    "ax = axs[0]\n",
    "ax.set_title(\"Average Losses\", fontsize=20)\n",
    "ax.plot(train_loss_history, label=\"Training\")\n",
    "ax.plot(val_loss_history,   label=\"Validation\")\n",
    "ax.set_xlabel(\"Epoch\",        fontsize=16)\n",
    "ax.set_ylabel(\"Average loss\", fontsize=16)\n",
    "ax.legend(loc=\"best\",         fontsize=16)\n",
    "\n",
    "ax = axs[1]\n",
    "ax.set_title(\"Accuracy\", fontsize=22)\n",
    "ax.plot(train_acc_history, label=\"Training\")\n",
    "ax.plot(val_acc_history,   label=\"Validation\")\n",
    "ax.set_xlabel(\"Epoch\",        fontsize=16)\n",
    "ax.set_ylabel(\"Accuracy\",     fontsize=16)\n",
    "ax.legend(loc=\"best\",         fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WkDYz8Qd62Mb"
   },
   "source": [
    "# Part 2: Building a better model\n",
    "\n",
    "Now, it's your turn to build a model. Your goal is to get an accuracy of at least 35% on the validation set. The only thing you can change is your model architecture. Think: is the model underfitting or overfitting?\n",
    "\n",
    "\n",
    "### <font color='red'>TODO </font>:\n",
    "In `student.py`, define your own `AnimalStudentNet` model architecture. We will be running your `model_train` with your `AnimalStudentNet` when grading to check consistency. Please make sure you follow the following restrictions when building your network. \n",
    "#### Restrictions ####\n",
    "* Model must be below 5 MB\n",
    "* You may not use pretrained models or model architectures from the internet. We are expecting you to build one from scratch. \n",
    "* You may not use any other training set than the one provided in the assignment. \n",
    "* You may not hand-label the test set. \n",
    "* You may not share models between groups. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ba6oA8Kl62Mc"
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(student)\n",
    "\n",
    "net = student.AnimalStudentNet()\n",
    "\n",
    "for layer in net.children():\n",
    "    print(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vb3IHE0w62Mc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Keep track of average losses, training accuracy and validation accuracy for each epoch\n",
    "train_loss_history = np.zeros(epochs)\n",
    "train_acc_history  = np.zeros(epochs)\n",
    "val_loss_history   = np.zeros(epochs)\n",
    "val_acc_history    = np.zeros(epochs)\n",
    "\n",
    "start_time = datetime.now()\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.01)\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "     # ============================ Training ==============================\n",
    "    print(\"Training...\")\n",
    "    \n",
    "    # Place network in training mode\n",
    "    net.train()\n",
    "    \n",
    "    # Initialize running epoch loss and number correctly classified\n",
    "    running_loss   = 0.0\n",
    "    num_correct    = 0.0\n",
    "    total_images   = 0.0\n",
    "    \n",
    "    for batch_num, (inputs, labels) in enumerate(train_dataloader):\n",
    "        # [inputs] and [labels] is one batch of images and their classes\n",
    "\n",
    "        \n",
    "        #  ***** Function call to student *****\n",
    "        curr_loss, curr_correct, curr_images = \\\n",
    "            student.model_train(net, inputs, labels, criterion, optimizer)\n",
    "        running_loss += curr_loss\n",
    "        num_correct += curr_correct\n",
    "        total_images += curr_images\n",
    "\n",
    "    # Update statistics for epoch\n",
    "    train_loss_history[epoch] = running_loss / total_images\n",
    "    train_acc_history[epoch]  = float(num_correct)  / float(total_images)\n",
    "    print(\"\\n Epoch: {:d} | Train Avg. Loss: [{}] | Acc: {} on {} images\\n\".format(epoch,\n",
    "          round(train_loss_history[epoch],4), train_acc_history[epoch], total_images) )\n",
    "    \n",
    "    # ============================ Validation ==============================\n",
    "    print(\"Validating...\")\n",
    "    # Place network in testing mode (won't need to keep track of gradients)\n",
    "    net.eval()\n",
    "    \n",
    "    running_loss   = 0.0\n",
    "    num_correct    = 0.0\n",
    "    total_images   = 0.0\n",
    "    \n",
    "    for batch_num, (inputs, labels) in enumerate(val_dataloader):\n",
    "        \n",
    "        # Propagate batch through network\n",
    "        outputs  = net(inputs)\n",
    "\n",
    "                                                 \n",
    "        # Calculate loss\n",
    "        loss     = criterion(outputs, labels.squeeze())\n",
    "                                                 \n",
    "        # Prediction is class with highest class score\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        running_loss  += loss.item()\n",
    "        num_correct   += torch.sum(preds == labels.data.reshape(-1))\n",
    "        total_images  += labels.data.numpy().size\n",
    "        \n",
    "    # Update statistics for validation data\n",
    "    val_loss_history[epoch] = running_loss / total_images\n",
    "    val_acc_history[epoch]  = float(num_correct)  / float(total_images) \n",
    "    print(\"Val Avg. Loss: [{}] Acc: {} on {} images\\n\".format(\n",
    "        round(val_loss_history[epoch],4), val_acc_history[epoch], total_images))\n",
    "    \n",
    "print(\"Time Elapsed: {} seconds\".format(\n",
    "    (datetime.now() - start_time).total_seconds()))\n",
    "\n",
    "\n",
    "\n",
    "torch.save(net.state_dict(), os.path.join('.','models','my_new_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "omPLaEnR62Mc"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(11,5))\n",
    "\n",
    "ax = axs[0]\n",
    "ax.set_title(\"Student Avg Losses\", fontsize=20)\n",
    "ax.plot(train_loss_history, label=\"Training\")\n",
    "ax.plot(val_loss_history,   label=\"Validation\")\n",
    "ax.set_xlabel(\"Epoch\",        fontsize=16)\n",
    "ax.set_ylabel(\"Average loss\", fontsize=16)\n",
    "ax.legend(loc=\"best\",         fontsize=16)\n",
    "\n",
    "ax = axs[1]\n",
    "ax.set_title(\"Student Accuracy\", fontsize=22)\n",
    "ax.plot(train_acc_history, label=\"Training\")\n",
    "ax.plot(val_acc_history,   label=\"Validation\")\n",
    "ax.set_xlabel(\"Epoch\",        fontsize=16)\n",
    "ax.set_ylabel(\"Accuracy\",     fontsize=16)\n",
    "ax.legend(loc=\"best\",         fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hfdvv_kr62Md"
   },
   "source": [
    "### Test accuracies ###\n",
    "Let us now compute test accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lWDf0iSg62Md",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net1 = student.AnimalBaselineNet()\n",
    "weights_path = os.path.join('.', 'models', 'my_baseline.pth')\n",
    "net1.load_state_dict(torch.load(weights_path, map_location=\"cpu\"))\n",
    "\n",
    "\n",
    "\n",
    "num_correct = 0\n",
    "total_images = 0\n",
    "running_loss = 0\n",
    "for batch_num, (inputs, labels) in enumerate(test_dataloader):\n",
    "        \n",
    "        # Propagate batch through network\n",
    "        outputs  = net1(inputs)\n",
    "                                                 \n",
    "        # Calculate loss\n",
    "        loss     = criterion(outputs, labels.squeeze())\n",
    "                                                 \n",
    "        # Prediction is class with highest class score\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        running_loss  += loss.item()\n",
    "        num_correct   += torch.sum(preds == labels.data.reshape(-1))\n",
    "        total_images  += labels.data.numpy().size\n",
    "\n",
    "print('Accuracy of baseline is {:f}'.format(float(num_correct)/float(total_images)))\n",
    "\n",
    "num_correct = 0\n",
    "total_images = 0\n",
    "running_loss = 0        \n",
    " \n",
    "net2 = student.AnimalStudentNet()\n",
    "weights_path = os.path.join('.', 'models', 'my_new_model.pth')\n",
    "net2.load_state_dict(torch.load(weights_path, map_location=\"cpu\"))\n",
    "\n",
    "for batch_num, (inputs, labels) in enumerate(test_dataloader):\n",
    "        \n",
    "        # Propagate batch through network\n",
    "        outputs  = net2(inputs)\n",
    "                                                 \n",
    "        # Calculate loss\n",
    "        loss     = criterion(outputs, labels.squeeze())\n",
    "                                                 \n",
    "        # Prediction is class with highest class score\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        running_loss  += loss.item()\n",
    "        num_correct   += torch.sum(preds == labels.data.reshape(-1))\n",
    "        total_images  += labels.data.numpy().size\n",
    "\n",
    "print('Accuracy of new model is {:f}'.format(float(num_correct)/float(total_images)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Os498g2a62Md"
   },
   "source": [
    "# Part 3: Adversarial Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D-SNXdB162Md"
   },
   "source": [
    "Convolutional neural networks are extremely powerful models when it comes to images; today, they are used extensively in drug discovery, disease detection, self-driving cars, and more. However, they are far from perfect. Turns out, these networks can be tricked very easily, using the concept of _adversarial examples_. In this part, we will generate adversarial examples to trick our own model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cX9bhR4a62Me"
   },
   "source": [
    "#### High-Level Overview\n",
    "We start with an image, $I$, that is classified correctly as class $C$. Our objective is to **manipulate the image** $I$ by adding **small changes** to it which would make the network **misclassify** the image. We can do so by observing the gradients produced within the model. Essentially, we find the direction in which we must change each input image pixel to maximize the loss, and we give the image's pixels a slight nudge in that direction. We get a new image, $I_{perturbed}$, that the network misclassifies.\n",
    "\n",
    "The scary part is that $I_{perturbed}$ contains tiny, imperceptible changes relative to $I$; to the human eye, $I$ and $I_{perturbed}$ visually look like the same image. Yet, the model perceives these two images completely differently.\n",
    "\n",
    "\n",
    "#### Implementation\n",
    "\n",
    "Below is a loop that loads the baseline model and sends images through it one by one. When it classifies an image correctly, you will create an adversarial image and see if you can fool the network into misclassifying it.\n",
    "\n",
    "\n",
    "For each correctly classified image, we need the gradient of the loss with respect to the image: $G = \\frac{\\partial_{loss}}{\\partial_{I}}$. We use this to create a matrix $\\alpha$ (defined below) of perturbations and add it to the original image:\n",
    "\n",
    "$$I_{perturbed} = I + \\alpha$$\n",
    "\n",
    "$\\alpha$ and $G$ have the same dimensions. $\\alpha$ is a matrix where the absolute value of each element is a small $\\epsilon$, yet the _sign_ (positive or negative) of $\\alpha[i,j]$ is equal to the _sign_ of $G[i,j]$. This makes sense: if the gradient value at some pixel is  negative, then you want to follow that negative slope and add negative noise value to your original image.\n",
    "\n",
    "\n",
    "### **<font color=\"red\">TODO : </font>**\n",
    "Complete the `get_adversarial` in `student.py`. Run the cells below. Your function should get **at least 50% of adversarial images misclassified** with an `epsilon=0.02`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yPJlVRCH62Me"
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(student)\n",
    "\n",
    "# Load pretrained baseline model\n",
    "net          = student.AnimalBaselineNet()\n",
    "weights_path = os.path.join('.', 'models', 'baseline.pth')\n",
    "net.load_state_dict(torch.load(weights_path, map_location=\"cpu\"))\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epsilon = 0.02\n",
    "\n",
    "original_imgs      = []\n",
    "original_preds     = []\n",
    "\n",
    "adversarial_imgs   = []\n",
    "adversarial_preds  = []\n",
    "adversarial_noises = []\n",
    "\n",
    "\n",
    "for i, (img, label) in enumerate(val_dataloader):\n",
    "    \n",
    "    # Set image tensor so gradient is calculated\n",
    "    img.requires_grad = True\n",
    "    \n",
    "    output = net(img)\n",
    "\n",
    "    init_pred = output.max(1, keepdim=True)[1]\n",
    "    \n",
    "    if init_pred == label:\n",
    "        # Image classified correctly; generate adversarial image\n",
    "        perturbed_img, noise = student.get_adversarial(img, output, label, net, criterion, epsilon)\n",
    "    \n",
    "        adversarial_output = net(perturbed_img)\n",
    "        adversarial_pred   = adversarial_output.max(1, keepdim=True)[1]\n",
    "        original_imgs.append(img[0].detach().numpy())\n",
    "        original_preds.append(init_pred)\n",
    "        adversarial_imgs.append(perturbed_img[0].detach().numpy())\n",
    "        adversarial_preds.append(adversarial_pred)\n",
    "        adversarial_noises.append(noise.squeeze().detach().numpy())\n",
    "    \n",
    "\n",
    "\n",
    "print(\"Out of total {} images generated, {} % of adversarial images misclassified\".format(\n",
    "    len(original_imgs),\n",
    "    round((torch.sum(torch.Tensor(original_preds) != torch.Tensor(adversarial_preds)).item())\n",
    "        / len(original_imgs) * 100., 4)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JCZFNwBS62Me"
   },
   "outputs": [],
   "source": [
    "num_desired_adversarial = 4\n",
    "\n",
    "fig, axs = plt.subplots(num_desired_adversarial, 3, figsize=(10,3*num_desired_adversarial))\n",
    "matplotlib.rcParams.update({'font.size': 14})\n",
    "\n",
    "def normalize(img):\n",
    "        img = img.astype(float)\n",
    "        img= img - np.min(img)\n",
    "        img = img / np.max(img)\n",
    "        return img\n",
    "\n",
    "\n",
    "for i in range(num_desired_adversarial):\n",
    "    ax = axs[i,0]\n",
    "    ax.set_title('Original: {}'.format(classes[original_preds[i]]))\n",
    "    ax.imshow(normalize(original_imgs[i].transpose(1,2,0)))\n",
    "    \n",
    "    ax = axs[i,1]\n",
    "    ax.set_title('Adversarial Noise')\n",
    "    ax.imshow(normalize(adversarial_noises[i].transpose(1,2,0)))\n",
    "    \n",
    "    ax = axs[i,2]\n",
    "    ax.set_title('Adversarial: {}'.format(classes[adversarial_preds[i]]))\n",
    "    ax.imshow(normalize(adversarial_imgs[i].transpose(1,2,0)))\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NA6HU-jO62Me"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "PA4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
